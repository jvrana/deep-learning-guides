import numpy as np
import torch
import dgl
from torch import nn
from matplotlib import pylab as plt


def get_complete_edges(g):
    def leading_zero(x):
        yield torch.zeros_like(x[0])
        yield from x
    x = torch.cumsum(g.batch_num_nodes(), 0)
    all_edges = []
    for a, b in zip(leading_zero(x), x):
        edges = torch.combinations(torch.arange(a, b))
        all_edges.append(edges)
    return torch.cat(all_edges, 0).T

def add_complete_edges(g):
    n1 = g.number_of_nodes()
    edges = get_complete_edges(g)
    g.add_edges(edges[0], edges[1])
    g.add_edges(edges[1], edges[0])
    n2 = g.number_of_nodes()
    assert n1 == n2
    return g


import pandas as pd

df = pd.read_csv('lucas0_train.csv')
df.head()


import typing
from copy import deepcopy


class Compose(typing.Callable):
    
    def __init__(self, *transforms):
        self.transforms = transforms
        
    def __call__(self, x):
        for transform in self.transforms:
            x = transform(x)
        return x
    
class Deepcopy(typing.Callable):
    
    def __call__(self, x):
        return deepcopy(x)

class ToDGLFullyConnected(typing.Callable):
    
    def __init__(self, keys, feature_key='feat', target_key='targ'):
        self.keys = keys
        self.key_to_idx = {k: i for i, k in enumerate(keys)}
        self.feature_key = feature_key
        self.target_key = target_key
        
    @staticmethod
    def leading_zero(x):
        yield torch.zeros_like(x[0])
        yield from x
        
    @classmethod
    def get_complete_edges(cls, g):
        x = torch.cumsum(g.batch_num_nodes(), 0)
        all_edges = []
        for a, b in zip(cls.leading_zero(x), x):
            edges = torch.combinations(torch.arange(a, b))
            all_edges.append(edges)
        return torch.cat(all_edges, 0).T

    @classmethod
    def add_complete_edges(cls, g):
        n1 = g.number_of_nodes()
        edges = cls.get_complete_edges(g)
        g.add_edges(edges[0], edges[1])
        g.add_edges(edges[1], edges[0])
        n2 = g.number_of_nodes()
        assert n1 == n2
        return g
        
    def __call__(self, row):
        g = dgl.graph(([], []))
        g.add_nodes(len(row))
        idx = [self.key_to_idx[k] for k in row.keys()]
        g.ndata[self.feature_key] = torch.tensor(idx).long()
        add_complete_edges(g)
        g.ndata[self.target_key] = torch.tensor(list(row.values())).unsqueeze(1).float()
        return g
    
class CloneNodeData(typing.Callable):
    
    def __init__(self, from_key: str, to_key: str):
        self.from_key = from_key
        self.to_key = to_key
        
    def __call__(self, g):
        g.ndata[self.to_key] = g.ndata[self.from_key].detach().clone()
        return g
        
class MaskNodeData(typing.Callable):
    
    def __init__(self, index: int, key: str, mask_key='m', mask_value=0):
        
        self.index = index
        self.key = key
        self.mask_key = mask_key
        self.mask_value = mask_value
        
        
    def __call__(self, g):
        mask = torch.zeros_like(g.ndata[self.key]).long()
        mask[self.index] = 1
        mask = mask.bool()
        g.ndata[self.mask_key] = mask
        g.ndata[self.key][mask] = self.mask_value
        return g
        
# t = Compose(
#     ToDGLFullyConnected(list(df.columns), feature_key='x', target_key='y'), 
#     CloneNodeData('y', 'y_hat'),
#     MaskNodeData(-1, key='y', mask_key='mask', mask_value=-1))
# t(row.to_dict())



from torch.utils.data import DataLoader, Dataset

class PdDataset(Dataset):
    
    def __init__(self, df, transforms=None):
        self.df = df
        self.transforms = transforms
        self.transformed = {}
        
    def __getitem__(self, idx):
        x = self.df.iloc[idx].to_dict()
        if self.transforms:
            if isinstance(self.transforms, (list, tuple)):
                for t in self.transforms:
                    x = t(x)
            elif callable(self.transforms):
                x = self.transforms(x)
            else:
                raise TypeError("Transforms must be callable or an list or tuple of callables")
        return x
    
    def __len__(self):
        return len(self.df)
    
    def split(self, *splits):
        x = torch.tensor(splits)
        x = torch.cumsum(x, 0) / x.sum()
        
        idx = len(self) * x
        idx = [0] + idx.long().tolist()
        idx[-1] = None
        idx[0] = None
        datasets = []
        for i, j in zip(idx[:-1], idx[1:]):
            datasets.append(self.__class__(self.df.iloc[i:j], transforms=self.transforms))
        return datasets 
        
class CachedDataset(Dataset):
    
    def __init__(self, dataset):
        self.cache = {}
        self.dataset = dataset
        
    def __getitem__(self, idx):
        if idx in self.cache:
            return self.cache[idx]
        else:
            x = self.dataset[idx]
            self.cache[idx] = x
            return x
        
    def __len__(self):
        return len(self.dataset)
    
    def split(self, *splits):
        return [self.__class__(s) for s in self.dataset.split(*splits)]
        
LeaveOneOut = Compose(
    Deepcopy(),
    ToDGLFullyConnected(list(df.columns), feature_key='x', target_key='y'), 
    CloneNodeData('y', 'y_hat'),
    MaskNodeData(-1, key='y', mask_key='mask', mask_value=-1)
)
dataset = PdDataset(df=pd.concat([df, df], ignore_index=True), transforms=LeaveOneOut)

dataset[3000]


from abc import ABC, abstractmethod
from torch import nn
from typing import *
import torch
from copy import deepcopy
import matplotlib as mpl
from matplotlib import cm
from dgl import ops as Fops
from torch.nn import functional as F


def clones(net, N):
    return [deepcopy(net) for _ in range(N)]
    
    
class SizedModule(ABC) :

    @abstractmethod
    def get_size(self) -> int:
        ...
                  
class AddNorm(nn.Module):
    
    def __init__(self, size: Optional[int] = None, dropout: float = 0.1, layer: Optional[SizedModule] = None):
        super().__init__()
        if size is None and layer is None:
            return ValueError("Either size or layer must be provided")
        self.size = size or layer.get_size()
        self.layer = layer
        self.norm = nn.LayerNorm(self.size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, args=None, kwargs=None, layer: Optional[SizedModule] = None):
        kwargs = kwargs or dict()
        if args is None:
            args = (x,)
        layer = layer or self.layer
        return self.norm(x + self.dropout(layer(*args, **kwargs)))
    

    
class MultiHeadAttention(nn.Module):
    
    def __init__(self, dim_model, h):
        super().__init__()
        assert dim_model % h == 0
        self.h = h
        self.dim_model = dim_model
        self.d_k = dim_model // h
        self.linears  = clones(nn.Linear(dim_model, dim_model), 4)
        self.attn = None
    
    def _view_head(self, x):
        return x.view(x.size(0), -1, self.h, self.d_k).transpose(1, 2)
        
    def forward(self, g, query, key, value):
        q = self._view_head(self.linears[0](query))
        k = self._view_head(self.linears[1](key))
        v = self._view_head(self.linears[2](value))
        x = Fops.v_dot_u(g, q, k) / self.d_k**0.5
        score = Fops.edge_softmax(g, x)
#         score = Fops.v_dot_u(g, q, k) / self.d_k**0.5
#         score = F.leaky_relu(Fops.v_dot_u(g, q, k) / self.d_k**0.5)
        out = Fops.u_mul_e_sum(g, v, score)
        out = out.transpose(1, 2).view(g.number_of_nodes(), self.h * self.d_k)
        score = score.view(score.size(0), self.h, -1)
        self.attn = score
        out = self.linears[3](out)
        return out
    

class Network(nn.Module):
    
    def __init__(self, d_model, h=16, n_heads=4, dropout=0.2):
        super().__init__()
        self.src_embedding = nn.Sequential(
            nn.Embedding(d_model, h),
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.dst_embedding = nn.Sequential(
            nn.Embedding(d_model, h),
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.encode = nn.Sequential(
            nn.Linear(1, h),
            nn.LeakyReLU(),
        )
        self.attn = AddNorm(h, layer=MultiHeadAttention(h, n_heads), dropout=dropout)
        self.core = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
        )
        self.decode = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, 1)
        )
            
        
    def forward(self, g, x, y, n_loops=5):
        with g.local_scope():
            g.ndata['a'] = self.src_embedding(x.flatten().long())
            g.ndata['b'] = self.dst_embedding(x.flatten().long())
            g.ndata['h'] = self.encode(y.float())
            out_arr = []
            for i in range(n_loops):

                g.ndata['h'] = self.attn(g.ndata['h'], args=(g, g.ndata['a'], g.ndata['b'], g.ndata['h']))
                g.ndata['h'] = self.core(g.ndata['h'])
                out = nn.Sigmoid()(self.decode(g.ndata['h']))
                out_arr.append(out)
            
            return out_arr
        
net = Network(d_model=len(df.columns)*2, h=128, n_heads=4, dropout=0.2)
g = dataset[0]
net(g, g.ndata['x'], g.ndata['y']);


from tqdm.auto import tqdm
from IPython import display


LeaveOneOut = Compose(
#     Deepcopy(),
    ToDGLFullyConnected(list(df.columns), feature_key='x', target_key='y'), 
    CloneNodeData('y', 'y_hat'),
    MaskNodeData(-1, key='y', mask_key='mask', mask_value=-1)
)
dataset = PdDataset(df=df, transforms=LeaveOneOut)
dataset = CachedDataset(dataset)
train_dataset, eval_dataset = dataset.split(0.8, 0.2)
datasets = {
    'train': train_dataset,
    'eval': eval_dataset,
    'full': dataset
}

loaders = {
    'train': DataLoader(datasets['train'], batch_size=32, collate_fn=dgl.batch, shuffle=True),
    'eval': DataLoader(datasets['eval'], batch_size=len(datasets['eval']), collate_fn=dgl.batch)
}

net = Network(d_model=len(df.columns)*2, h=32, n_heads=4, dropout=0.2)
optim = torch.optim.AdamW(net.parameters(), lr=1e-3)
lossfn = torch.nn.BCELoss()


def compute_loss(net, g):
    m = g.ndata['mask']
    y_arr = net(g, g.ndata['x'], g.ndata['y'])
    y_hat = g.ndata['y_hat']
    losses = []
    for y in y_arr:
        losses.append(lossfn(y[m], y_hat[m]))
    loss = torch.stack(losses)
    return loss.mean()

def compute_train_loss(net, g):
    net.train()
    return compute_loss(net, g)

def compute_eval_loss(net, g):
    net.eval()
    with torch.no_grad():
        return compute_loss(net, g)

n_epochs = 100
train_losses = []
eval_losses = []
for epoch in tqdm(range(n_epochs)):
    
    # train
    epoch_train_loss = []
    for g in loaders['train']:
        train_loss = compute_train_loss(net, g)
        optim.zero_grad()
        train_loss.backward()
        optim.step()
        epoch_train_loss.append(train_loss.detach().item())
    epoch_train_loss = torch.tensor(epoch_train_loss)
    train_losses.append((epoch, epoch_train_loss.mean()))
        
    # evaluate
    if epoch % 1 == 0:
        epoch_eval_loss = []
        for g in loaders['eval']:
            eval_loss = compute_eval_loss(net, g)
            epoch_eval_loss.append(eval_loss.detach().item())
        epoch_eval_loss = torch.tensor(epoch_eval_loss)
        eval_losses.append((epoch, epoch_eval_loss.mean()))
        
    # plot
    display.clear_output(wait=True)
    fig, axes = plt.subplots(2, 1, figsize=(10, 8))
    epoch, loss = zip(*train_losses)
    axes[0].plot(epoch, loss)
    
    epoch, loss = zip(*eval_losses)
    axes[1].plot(epoch, loss)
    plt.show()
        
        


import seaborn as sns

with torch.no_grad():
    net.eval()
    for g in loaders['eval']:
        ...
    m = g.ndata['mask']
    y = net(g, g.ndata['x'], g.ndata['y'])[-1][m]
    y_hat = g.ndata['y_hat'][m]
    
false_pos = (y>=0.5)[~y_hat.bool()].sum() / (~y_hat.bool()).sum()
true_pos = (y>=0.5)[y_hat.bool()].sum() / (y_hat.bool()).sum()
false_neg = (y<0.5)[y_hat.bool()].sum() / (y_hat.bool()).sum()
true_neg = (y<0.5)[~y_hat.bool()].sum() / (~y_hat.bool()).sum()
error = (torch.logical_xor(y >= 0.5, y_hat.bool())).sum() / len(y)
print("Classification error: {}".format(error))
print("Sensitivity (true pos): {}".format(true_pos))
print("Specificity (true neg): {}".format(true_neg))

perc = (~torch.logical_xor(y_hat.bool(), y > 0.5)).sum() / len(y_hat)
print("Percentage correct: {}".format(perc.item()))
sns.heatmap(torch.stack([y, y_hat]).detach().numpy().T)
plt.show()


import seaborn as sns
import networkx as nx


def to_nxg(g: dgl.DGLGraph) -> nx.DiGraph:
    nxg = nx.DiGraph(g.to_networkx())
    return nxg
    
def plot_graph(g: dgl.DGLGraph, ax=None, prog='neato', **kwargs):
    nxg = to_nxg(g)
    pos = nx.nx_agraph.pygraphviz_layout(nxg, prog=prog)
    nx.draw(nxg, pos=pos, ax=ax, **kwargs)
    return ax, pos, nxg

def attn_to_sparse(g: dgl.DGLGraph, attn: torch.Tensor):
    n = g.number_of_nodes()
    i = torch.stack(g.edges())
    s = attn.shape
    expected_shape = (n, n, *list(s)[1:])
    x = torch.sparse_coo_tensor(i, attn, expected_shape)
    return x


def plot_multihead_attention(g, attn, cmap='binary', labels='auto', node_color='k', node_size=30, prog='neato', scale_width=2., min_width=0.3):
    fig, axes = plt.subplots(2, 4, figsize=(14, 5))
    attn_arr =[]
    
    attn_ = attn_to_sparse(g, attn.detach().squeeze(-1)).to_dense()
    for i in range(attn_.shape[-1]):
        ax = axes.flatten()[i]
        ax.set_title("Head {}".format(i))
        _attn = attn_[..., i]
        attn_arr.append(_attn)
        sns.heatmap(_attn, ax=ax, linewidths=0, cmap=cmap, xticklabels=labels, yticklabels=labels)
        norm = mpl.colors.Normalize(vmin=0,vmax=1.)
        a = _attn.flatten()
        edge_colors = cm.get_cmap(cmap)(norm(a))
        edges = g.edges()
        edgelist = [(edges[0][i].item(), edges[1][i].item()) for i in range(g.number_of_edges())]
        
        ax, pos, nxg = plot_graph(g, prog=prog, width=a*scale_width + min_width, edge_color=edge_colors, edgelist=edgelist, ax=axes.flatten()[i+4], node_size=node_size, node_color=node_color)
    plt.tight_layout()
    
def eval_network(g, net):
    net(g, g.ndata['x'], g.ndata['y'])
    attn = net.attn.layer.attn
    plot_multihead_attention(g, attn, labels=list(df.columns))
    
g = loaders['eval'].dataset[2]
net(g, g.ndata['x'], g.ndata['y'])
attn = net.attn.layer.attn.squeeze(-1)
fig, ax = plt.subplots(1, 1, figsize=(5, .5))
sns.heatmap(attn_to_sparse(g, attn).to_dense().mean(-1)[..., -1:].detach().T, ax=ax, cmap='binary', yticklabels=[''], xticklabels=df.columns)
plt.show()

eval_network(loaders['eval'].dataset[2], net)


class AddIncomingEdges(typing.Callable):
    
    def __init__(self, embed_key):
        self.embed_key = embed_key
        
    def __call__(self, g):
        n = g.number_of_nodes()
        g.add_nodes(n)
        g.ndata[self.embed_key][n:] = torch.arange(n, 2*n)
        edges = torch.stack([
            torch.arange(n, 2*n),
            torch.arange(0, n),
        ])
        g.add_edges(edges[0], edges[1])
        return g
    
t = Compose(Deepcopy(), AddIncomingEdges('x'))


from tqdm.auto import tqdm
from IPython import display


LeaveOneOutIncoming = Compose(
    Deepcopy(),
    ToDGLFullyConnected(list(df.columns), feature_key='x', target_key='y'), 
    CloneNodeData('y', 'y_hat'),
    MaskNodeData(-1, key='y', mask_key='mask', mask_value=-1),
    AddIncomingEdges('x'),
)
dataset = PdDataset(df=df, transforms=LeaveOneOutIncoming)
dataset = CachedDataset(dataset)
train_dataset, eval_dataset = dataset.split(0.8, 0.2)
datasets = {
    'train': train_dataset,
    'eval': eval_dataset,
    'full': dataset
}

loaders = {
    'train': DataLoader(datasets['train'], batch_size=32, collate_fn=dgl.batch, shuffle=True),
    'eval': DataLoader(datasets['eval'], batch_size=len(datasets['eval']), collate_fn=dgl.batch)
}

net = Network(d_model=len(df.columns)*2, h=32, n_heads=4, dropout=0.2)
optim = torch.optim.AdamW(net.parameters(), lr=1e-3)
lossfn = torch.nn.BCELoss()


def compute_loss(net, g):
    m = g.ndata['mask']
    y_arr = net(g, g.ndata['x'], g.ndata['y'])
    y_hat = g.ndata['y_hat']
    losses = []
    for y in y_arr:
        losses.append(lossfn(y[m], y_hat[m]))
    loss = torch.stack(losses)
    return loss.mean()

def compute_train_loss(net, g):
    net.train()
    return compute_loss(net, g)

def compute_eval_loss(net, g):
    net.eval()
    with torch.no_grad():
        return compute_loss(net, g)

n_epochs = 100
train_losses = []
eval_losses = []
for epoch in tqdm(range(n_epochs)):
    
    # train
    epoch_train_loss = []
    for g in loaders['train']:
        train_loss = compute_train_loss(net, g)
        optim.zero_grad()
        train_loss.backward()
        optim.step()
        epoch_train_loss.append(train_loss.detach().item())
    epoch_train_loss = torch.tensor(epoch_train_loss)
    train_losses.append((epoch, epoch_train_loss.mean()))
        
    # evaluate
    if epoch % 1 == 0:
        epoch_eval_loss = []
        for g in loaders['eval']:
            eval_loss = compute_eval_loss(net, g)
            epoch_eval_loss.append(eval_loss.detach().item())
        epoch_eval_loss = torch.tensor(epoch_eval_loss)
        eval_losses.append((epoch, epoch_eval_loss.mean()))
        
    # plot
    display.clear_output(wait=True)
    fig, axes = plt.subplots(2, 1, figsize=(10, 8))
    epoch, loss = zip(*train_losses)
    axes[0].plot(epoch, loss)
    
    epoch, loss = zip(*eval_losses)
    axes[1].plot(epoch, loss)
    plt.show()
        
        


g = loaders['eval'].dataset[2]
net(g, g.ndata['x'], g.ndata['y'])
attn = net.attn.layer.attn.squeeze(-1)
fig, ax = plt.subplots(1, 1, figsize=(5, .5))
sns.heatmap(attn_to_sparse(g, attn).to_dense().mean(-1)[..., 11:12].detach().T, ax=ax, cmap='binary', yticklabels=[''], xticklabels=df.columns)
plt.show()

eval_network(loaders['eval'].dataset[2], net)


        
class RandomMaskNodeData(typing.Callable):
    
    def __init__(self, key: str, mask_key='m', mask_value=0):
        self.key = key
        self.mask_key = mask_key
        self.mask_value = mask_value
        
        
    def __call__(self, g):
        mask = torch.zeros_like(g.ndata[self.key]).long()
        i = torch.randint(0, len(mask), (1,))
        mask[i] = 1
        mask = mask.bool()
        g.ndata[self.mask_key] = mask
        g.ndata[self.key][mask] = self.mask_value
        return g
        



    
class MultiHeadAttention(nn.Module):
    
    def __init__(self, dim_model, h):
        super().__init__()
        assert dim_model % h == 0
        self.h = h
        self.dim_model = dim_model
        self.d_k = dim_model // h
        self.linears  = clones(nn.Linear(dim_model, dim_model), 4)
        self.attn = None
    
    def _view_head(self, x):
        return x.view(x.size(0), -1, self.h, self.d_k).transpose(1, 2)
        
    def forward(self, g, query, key, value):
        q = self._view_head(self.linears[0](query))
        k = self._view_head(self.linears[1](key))
        v = self._view_head(self.linears[2](value))
        x = Fops.v_dot_u(g, q, k) / self.d_k**0.5
        score = Fops.edge_softmax(g, x)
#         score = Fops.v_dot_u(g, q, k) / self.d_k**0.5
#         score = F.leaky_relu(Fops.v_dot_u(g, q, k) / self.d_k**0.5)
        out = Fops.u_mul_e_sum(g, v, score)
        out = out.transpose(1, 2).view(g.number_of_nodes(), self.h * self.d_k)
        score = score.view(score.size(0), self.h, -1)
        self.attn = score
        out = self.linears[3](out)
        return out
    
    
class Network2(nn.Module):
    
    def __init__(self, d_model, h=16, n_heads=4, dropout=0.2):
        super().__init__()
        self.embedding = nn.Embedding(d_model, h)
        self.dst_linears = nn.Sequential(
            nn.Linear(h+1, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.src_linears = nn.Sequential(
            nn.Linear(h+1, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.encode = nn.Sequential(
            nn.Linear(1, h),
            nn.LeakyReLU(),
        )
        self.attn = AddNorm(h, layer=MultiHeadAttention(h, n_heads), dropout=dropout)
        self.self_attn = AddNorm(h, layer=MultiHeadAttention(h, n_heads), dropout=dropout)
        linear = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
        )
        self.core = AddNorm(h, layer=linear, dropout=dropout)
        self.decode = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, 1)
        )
        self.l = nn.Sequential(
            nn.Linear(1, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU(),
        )
            
        
    def forward(self, g, x, y, m, n_loops=5):
        with g.local_scope():
            x = self.embedding(x.flatten().long())
            x = torch.cat([x, m], 1)
            a = self.src_linears(x)
            b = self.dst_linears(x)
            g.ndata['a'] = a
            g.ndata['b'] = b
            g.ndata['h'] = self.encode(y.float())
            out_arr = []
            for i in range(n_loops):
#                 g.ndata['h'] = self.self_attn(g.ndata['h'], args=(g, g.ndata['h'], g.ndata['h'], g.ndata['h']))
                g.ndata['h'] = self.attn(g.ndata['h'], args=(g, g.ndata['a'], g.ndata['b'], g.ndata['h']))
                g.ndata['h'] = self.core(g.ndata['h'])
                out = nn.Sigmoid()(self.decode(g.ndata['h']))
                out_arr.append(out)
            
            return out_arr
        
net = Network2(d_model=len(df.columns)*2, h=128, n_heads=4, dropout=0.2)
g = dataset[0]
net(g, g.ndata['x'], g.ndata['y'], g.ndata['mask']);


from tqdm.auto import tqdm
from IPython import display


LeaveOneOutIncoming = Compose(
    Deepcopy(),
    ToDGLFullyConnected(list(df.columns), feature_key='x', target_key='y'), 
    CloneNodeData('y', 'y_hat'),
    RandomMaskNodeData(key='y', mask_key='mask', mask_value=-1),
    AddIncomingEdges('x'),
)
dataset = PdDataset(df=pd.concat([df], ignore_index=True), transforms=LeaveOneOutIncoming)
# dataset = CachedDataset(dataset)
train_dataset, eval_dataset = dataset.split(0.8, 0.2)
datasets = {
    'train': train_dataset,
    'eval': eval_dataset,
    'full': dataset
}

loaders = {
    'train': DataLoader(datasets['train'], batch_size=128, collate_fn=dgl.batch, shuffle=True),
    'eval': DataLoader(datasets['eval'], batch_size=len(datasets['eval']), collate_fn=dgl.batch)
}

net = Network2(d_model=len(df.columns)*2, h=32, n_heads=1, dropout=0.2)
optim = torch.optim.AdamW(net.parameters(), lr=1e-3)
lossfn = torch.nn.BCELoss()


def compute_loss(net, g):
    m = g.ndata['mask']
    y_arr = net(g, g.ndata['x'], g.ndata['y'], g.ndata['mask'], n_loops=3)
    y_hat = g.ndata['y_hat']
    losses = []
    for y in y_arr:
        losses.append(lossfn(y[m], y_hat[m]))
    loss = torch.stack(losses)
    return loss.mean()

def compute_train_loss(net, g):
    net.train()
    return compute_loss(net, g)

def compute_eval_loss(net, g):
    net.eval()
    with torch.no_grad():
        return compute_loss(net, g)

n_epochs = 1000
train_losses = []
eval_losses = []
device = 'cpu'
net.to(device)
for epoch in tqdm(range(n_epochs)):
    
    # evaluate
    if epoch % 1 == 0:
        epoch_eval_loss = []
        for g in loaders['eval']:
            g = g.to(device)
            eval_loss = compute_eval_loss(net, g)
            epoch_eval_loss.append(eval_loss.detach().item())
        epoch_eval_loss = torch.tensor(epoch_eval_loss)
        eval_losses.append((epoch, epoch_eval_loss.mean()))
        
    # train
    epoch_train_loss = []
    for g in loaders['train']:
        g = g.to(device)
        train_loss = compute_train_loss(net, g)
        optim.zero_grad()
        train_loss.backward()
        optim.step()
        epoch_train_loss.append(train_loss.detach().item())
    epoch_train_loss = torch.tensor(epoch_train_loss)
    train_losses.append((epoch, epoch_train_loss.mean()))
        
    # plot
    display.clear_output(wait=True)
    fig, axes = plt.subplots(3, 1, figsize=(10, 8))
    epoch, loss = zip(*train_losses)
    axes[0].plot(epoch, loss)
    
    epoch, loss = zip(*eval_losses)
    axes[1].plot(epoch, loss)
    
    g = loaders['eval'].dataset[2]
    net(g, g.ndata['x'], g.ndata['y'], g.ndata['mask'])
    attn = net.attn.layer.attn.squeeze(-1).detach()
    attn = attn_to_sparse(g, attn).to_dense()
    x = attn.mean(-1)[:, :12]

    sns.heatmap(x, cmap='binary', xticklabels=list(df.columns), yticklabels=list(df.columns), ax=axes[2])
    plt.show()
    
        
        


g = loaders['eval'].dataset[2]
net(g, g.ndata['x'], g.ndata['y'], g.ndata['mask'])
attn = net.attn.layer.attn.squeeze(-1).detach()
attn = attn_to_sparse(g, attn).to_dense()
x = attn.mean(-1)[:, :12]

sns.heatmap(x, cmap='binary', xticklabels=list(df.columns), yticklabels=list(df.columns))



sns.heatmap((x.T - x) / (x.T - x).mean(), cmap='binary', xticklabels=list(df.columns), yticklabels=list(df.columns))


eval_network(loaders['eval'].dataset[2], net)


import random
class AddIncomingEdges2(typing.Callable):
    
    def __init__(self, embed_key, length):
        self.embed_key = embed_key
        self.length = length
        
    def __call__(self, g):
        n = g.number_of_nodes()
        g.add_nodes(n)
        g.ndata[self.embed_key][n:] = g.ndata[self.embed_key][:n] + self.length
        edges = torch.stack([
            torch.arange(n, 2*n),
            torch.arange(0, n),
        ])
        g.ndata['y'][:] = 1
        g.ndata['y_hat'][n:] = 1
        g.add_edges(edges[0], edges[1])
        return g
    
    
class AddCompleteEdges(typing.Callable):
    
    @staticmethod
    def leading_zero(x):
        yield torch.zeros_like(x[0])
        yield from x
        
    @classmethod
    def get_complete_edges(cls, g):
        x = torch.cumsum(g.batch_num_nodes(), 0)
        all_edges = []
        for a, b in zip(cls.leading_zero(x), x):
            edges = torch.combinations(torch.arange(a, b))
            all_edges.append(edges)
        return torch.cat(all_edges, 0).T

    @classmethod
    def add_complete_edges(cls, g):
        n1 = g.number_of_nodes()
        edges = cls.get_complete_edges(g)
        g.add_edges(edges[0], edges[1])
        g.add_edges(edges[1], edges[0])
        n2 = g.number_of_nodes()
        assert n1 == n2
        return g
    
    def __call__(self, g):
        self.add_complete_edges(g)
        return g

class ToLocalEventsGraph(typing.Callable):
    
    def __init__(self, events):
        self.events = list(events)
        self.events_to_index = {e: i for i, e in enumerate(events)}
        
    def from_events(self, local_events, target_event, target_value):
        
        local_events = local_events[:]
        target_event = random.choice(self.events)
        if target_event not in local_events:
            local_events.append(target_event)
        target_idx = local_events.index(target_event)

        g = dgl.graph(([], []))
        g.add_nodes(len(local_events))

        g.ndata['x'] = torch.tensor([event_to_index[e] for e in local_events])
        m = torch.zeros_like(g.ndata['x'])
        m[target_idx] = 1
        m = m.bool()
        g.ndata['mask'] = m
        g.ndata['y'] = torch.ones_like(g.ndata['x'].unsqueeze(-1)).float()
        g.ndata['y_hat'] = g.ndata['y'].detach().clone()
        g.ndata['y_hat'][m] = target_value
        return g
        
    def __call__(self, row):
        local_events = []
        for k, v in dict(row).items():
            if v == 1:
                local_events.append(k)

        target_event = random.choice(events)
        target_value = dict(row)[target_event]
        return self.from_events(local_events, target_event, target_value)


transform = Compose(
    ToLocalEventsGraph(events=df.columns),
    AddCompleteEdges(),
    AddIncomingEdges2('x', len(df.columns))
)
g = transform(row)
print(g)
print(g.ndata)


class Network3(nn.Module):
    
    def __init__(self, d_model, h=32, n_heads=4, dropout=0.2):
        super().__init__()
        self.embedding = nn.Embedding(d_model, h)
        self.dst_linears = nn.Sequential(
            nn.Linear(h+1, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.src_linears = nn.Sequential(
            nn.Linear(h+1, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.encode = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
        )
        self.attn = AddNorm(h, layer=MultiHeadAttention(h, n_heads), dropout=dropout)
        self.self_attn = AddNorm(h, layer=MultiHeadAttention(h, n_heads), dropout=dropout)
        linear = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
        )
        self.core = AddNorm(h, layer=linear, dropout=dropout)
        self.decode = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, 1)
        )
            
        
    def forward(self, g, x, y, m, n_loops=5):
        with g.local_scope():
            x = self.embedding(x.flatten().long())
            x = torch.cat([x, m.float().unsqueeze(1)], 1)
            a = self.src_linears(x)
            b = self.dst_linears(x)
            g.ndata['a'] = a
            g.ndata['b'] = b
            g.ndata['h'] = self.encode(torch.ones(a.shape))
            out_arr = []
            for i in range(n_loops):
                g.ndata['h'] = self.attn(g.ndata['h'], args=(g, g.ndata['a'], g.ndata['b'], g.ndata['h']))
                g.ndata['h'] = self.core(g.ndata['h'])
                out = nn.Sigmoid()(self.decode(g.ndata['h']))
                out_arr.append(out)
            return out_arr
        
transform = Compose(
    ToLocalEventsGraph(events=df.columns),
    AddCompleteEdges(),
    AddIncomingEdges2('x', len(df.columns))
)
g = transform(row)
net3 = Network3(len(df.columns)*2)
net3(g, g.ndata['x'], g.ndata['y'], g.ndata['mask'])[-1]


from tqdm.auto import tqdm
from IPython import display


LocalEvents = Compose(
    ToLocalEventsGraph(events=df.columns),
    AddCompleteEdges(),
    AddIncomingEdges2('x', length=len(df.columns))
)
dataset = PdDataset(df=pd.concat([df, df, df, df, df, df], ignore_index=True), transforms=LocalEvents)
dataset = CachedDataset(dataset)
train_dataset, eval_dataset = dataset.split(0.8, 0.2)
datasets = {
    'train': train_dataset,
    'eval': eval_dataset,
    'full': dataset
}

loaders = {
    'train': DataLoader(datasets['train'], batch_size=128, collate_fn=dgl.batch, shuffle=True),
    'eval': DataLoader(datasets['eval'], batch_size=len(datasets['eval']), collate_fn=dgl.batch)
}

net = Network3(d_model=len(df.columns)*2, h=32, n_heads=4, dropout=0.2)
for g in loaders['train']:
    net(g, g.ndata['x'], g.ndata['y'], g.ndata['mask'])
    break
optim = torch.optim.AdamW(net.parameters(), lr=1e-3)
lossfn = torch.nn.BCELoss()


def compute_loss(net, g):
    m = g.ndata['mask']
    y_arr = net(g, g.ndata['x'], g.ndata['y'], g.ndata['mask'], n_loops=3)
    y_hat = g.ndata['y_hat']
    losses = []
    for y in y_arr[-1:]:
        losses.append(lossfn(y[m], y_hat[m]))
    loss = torch.stack(losses)
    return loss.mean()

def compute_train_loss(net, g):
    net.train()
    return compute_loss(net, g)

def compute_eval_loss(net, g):
    net.eval()
    with torch.no_grad():
        return compute_loss(net, g)

n_epochs = 100
train_losses = []
eval_losses = []
for epoch in tqdm(range(n_epochs)):
    
    # evaluate
    if epoch % 1 == 0:
        epoch_eval_loss = []
        for g in loaders['eval']:
            eval_loss = compute_eval_loss(net, g)
            epoch_eval_loss.append(eval_loss.detach().item())
        epoch_eval_loss = torch.tensor(epoch_eval_loss)
        eval_losses.append((epoch, epoch_eval_loss.mean()))
        
    # train
    epoch_train_loss = []
    for g in loaders['train']:
        train_loss = compute_train_loss(net, g)
        optim.zero_grad()
        train_loss.backward()
        optim.step()
        epoch_train_loss.append(train_loss.detach().item())
    epoch_train_loss = torch.tensor(epoch_train_loss)
    train_losses.append((epoch, epoch_train_loss.mean()))
        
    # plot
    display.clear_output(wait=True)
    fig, axes = plt.subplots(2, 1, figsize=(10, 8))
    epoch, loss = zip(*train_losses)
    axes[0].plot(epoch, loss)
    
    epoch, loss = zip(*eval_losses)
    axes[1].plot(epoch, loss)
    plt.show()
        
        


g.ndata['mask']


g = ToLocalEventsGraph(df.columns).from_events(list(df.columns), 'Lung_cancer', 1)
g = AddCompleteEdges()(g)
g = AddIncomingEdges2('x', len(df.columns))(g)

net(g, g.ndata['x'], g.ndata['y'], g.ndata['mask'])
attn = net.attn.layer.attn.squeeze(-1).detach()
attn = attn_to_sparse(g, attn).to_dense()
x = attn.mean(-1)[:, :12]

sns.heatmap(x, cmap='binary', xticklabels=list(df.columns), yticklabels=list(df.columns))


g = ToLocalEventsGraph(df.columns).from_events(list(df.columns), 'Smoking', 1)
g = AddCompleteEdges()(g)
g = AddIncomingEdges2('x', len(df.columns))(g)

x = net(g, g.ndata['x'], torch.ones_like(g.ndata['y']), g.ndata['mask'])

print(x[-1][g.ndata['mask']])
g.ndata['y_hat'][g.ndata['mask']]


g = loaders['eval'].dataset[2]
out = net(g, g.ndata['x'], g.ndata['y'], g.ndata['mask'])[-1]
attn = net.attn.layer.attn.squeeze(-1).detach()
attn = attn_to_sparse(g, attn).to_dense()
x = attn.mean(-1)[:, :12]

print(out[g.ndata['mask']])
print(g.ndata['y_hat'][g.ndata['mask']])

sns.heatmap(x, cmap='binary', xticklabels=list(df.columns), yticklabels=list(df.columns))


from torch.nn import functional as F
from tqdm.auto import tqdm
from IPython import display

class GumbelSoftMaxSampler(nn.Module):
    
    def __init__(self, hard=False):
        super().__init__()
        self.hard = hard
        
    def forward(self, logits):
        return F.gumbel_softmax(logits=logits, hard=self.hard)

    
class Parallel(nn.Module):
    
    def __init__(self, *modules):
        super().__init__()
        self.mods = nn.ModuleList(modules)
        self.n_modules = len(modules)
        
    def forward(self, x):
        assert x.shape[-1] == len(self.mods)
        out = []
        for i, mod in enumerate(self.mods):
            _x = x[..., i]
            try:
                result = mod(_x)
            except Exception as e:
                msg = "Error found in forward prop of module {}, {}\n".format(i, str(mod)[:1000])
                msg = msg + str(e)
                raise e.__class__(msg)
            out.append(result)
        return torch.cat(out, 1)
    
    
class ParallelEmbedding(Parallel):
    
    def __init__(self, *dims):
        mods = []
        for a, b in dims:
            mods.append(nn.Embedding(a, b))
        super().__init__(*mods)
        
    
class Dense(nn.Module):
    
    def __init__(self, *dims, dropout=0.2):
        super().__init__()
        layers = []
        for a, b in zip(dims[:-1], dims[1:]):
            layers += [nn.Linear(a, b), nn.LeakyReLU(), nn.Dropout(dropout)]
        self.layers = nn.Sequential(*layers)
        
    def __call__(self, x):
        return self.layers(x)
    
    
x = torch.randn(10, 12)
encoder = Dense(12, 16, 16, 2)
sampler = GumbelSoftMaxSampler()
decoder = Dense(2, 16, 16, 12)

x = torch.randn(10, 12)
logits = encoder(x)
y = sampler(logits)
x = decoder(y)
print(x.shape)

class GenerativeNetwork(nn.Module):
    
    def __init__(self, d_model, h=32, n_heads=4, dropout=0.2):
        super().__init__()
        
        # embedding for src nodes
        self.src_embed = nn.Sequential(
            nn.Embedding(d_model, h),
            Dense(h, h, dropout=dropout)
        )
        
        # embedding for dst nodes
        self.dst_embed = nn.Sequential(
            nn.Embedding(d_model, h),
            Dense(h, h, dropout=dropout)
        )

        # embedding to parameterize the "combining" function for inputs
        self.embed = nn.Sequential(
            ParallelEmbedding((d_model, h), (2, 1)),
            Dense(h+1, h, dropout=dropout)
        )

        # encode incoming data
        self.encoder = nn.Sequential(
            Dense(h + 2, h, dropout=dropout)
        )
        
        # attention to determine interactions
        self.attn = AddNorm(h, layer=MultiHeadAttention(h, n_heads), dropout=dropout)

        # decode to logits
        self.decoder = nn.Sequential(
            Dense(h, h),
            nn.Linear(h, 2)
        )
        
        # sample from logits
        self.sampler = GumbelSoftMaxSampler(hard=True)

    def forward(self, g, feature, target, feature2, n_loops=3):
        with g.local_scope():
            q = self.src_embed(feature)
            k = self.dst_embed(feature)
            
            m = torch.stack([feature, feature2], -1)
            m = self.embed(m)
            x = target
            out = [x]
            for i in range(n_loops):
                h = torch.cat([x, m], 1)
                v = self.encoder(h)
                h = self.attn(v, args=(g, q, k, v))
                logits = self.decoder(h)
#                 x = self.sampler(logits)
                x = nn.Sigmoid()(logits)
                out.append(x)
            return out

gennet = GenerativeNetwork(len(df.columns)*2)

g = dgl.graph(([0, 1, 2], [0, 2, 3]))
feature = torch.randint(0, 3, (g.number_of_nodes(),))
feature2 = torch.randint(0, 1, (g.number_of_nodes(),))
target = torch.randn(g.number_of_nodes(), 2)
gennet(g, feature, target, feature2)


def one_hot(x: torch.Tensor, num_classes: int, device=None, dtype=torch.long):
    to_shape = None
    if len(x.shape) > 1:
        to_shape = tuple(x.shape) + (num_classes,)
        x = x.flatten()
    b = torch.zeros(x.shape[0], num_classes, device=device, dtype=dtype)
    b[torch.arange(x.shape[0], device=device), x.to(device)] = 1
    if to_shape:
        b = b.view(to_shape)
    return b

class OneHotNode(typing.Callable):
    
    def __init__(self, key, num_classes, out_key=None, dtype=torch.long):
        self.num_classes = num_classes
        self.key = key
        if out_key is None:
            self.out_key = self.key
        self.dtype = dtype
        
    def __call__(self, g):
        g.ndata[self.out_key] = one_hot(g.ndata[self.key].long(), self.num_classes, dtype=self.dtype)
        return g
        

class NodeSqueeze(typing.Callable):
    
    def __init__(self, key, dim):
        self.dim = dim
        self.key = key
        
    def __call__(self, g):
        g.ndata[self.key] = g.ndata[self.key].squeeze(self.dim)
        return g

Transforms = Compose(
    Deepcopy(),
    ToDGLFullyConnected(list(df.columns), feature_key='x', target_key='y'), 
    CloneNodeData('y', 'y_hat'),
    RandomMaskNodeData(key='y', mask_key='mask', mask_value=1),
    AddIncomingEdges('x'),
    OneHotNode('y', 2, dtype=torch.float),
    OneHotNode('y_hat', 2, dtype=torch.float),
    NodeSqueeze('y', 1),
    NodeSqueeze('y_hat', 1)
)
dataset = PdDataset(df=pd.concat([df], ignore_index=True), transforms=Transforms)
# dataset = CachedDataset(dataset)
train_dataset, eval_dataset = dataset.split(0.8, 0.2)
datasets = {
    'train': train_dataset,
    'eval': eval_dataset,
    'full': dataset
}

loaders = {
    'train': DataLoader(datasets['train'], batch_size=128, collate_fn=dgl.batch, shuffle=True),
    'eval': DataLoader(datasets['eval'], batch_size=len(datasets['eval']), collate_fn=dgl.batch)
}


net = gennet = GenerativeNetwork(len(df.columns)*2)
optim = torch.optim.AdamW(net.parameters(), lr=1e-3)
lossfn = torch.nn.BCELoss()

for g in loaders['eval']:
    net(g, g.ndata['x'].flatten(), g.ndata['y'], g.ndata['mask'].long().flatten())
    
    
def compute_loss(net, g):
    m = g.ndata['mask'].flatten()
    y_arr = net(g, g.ndata['x'].view(-1), g.ndata['y'], g.ndata['mask'].long().view(-1), n_loops=10)
    y_hat = g.ndata['y_hat']
    losses = []
    for y in y_arr:
        losses.append(lossfn(y[m], y_hat[m]))
    loss = torch.stack(losses)
    return loss.mean()


def compute_train_loss(net, g):
    net.train()
    return compute_loss(net, g)


def compute_eval_loss(net, g):
    net.eval()
    with torch.no_grad():
        return compute_loss(net, g)

n_epochs = 1000
train_losses = []
eval_losses = []
device = 'cpu'
net.to(device)
for epoch in tqdm(range(n_epochs)):
    
    # evaluate
    if epoch % 1 == 0:
        epoch_eval_loss = []
        for g in loaders['eval']:
            g = g.to(device)
            eval_loss = compute_eval_loss(net, g)
            epoch_eval_loss.append(eval_loss.detach().item())
        epoch_eval_loss = torch.tensor(epoch_eval_loss)
        eval_losses.append((epoch, epoch_eval_loss.mean()))
        
    # train
    epoch_train_loss = []
    for g in loaders['train']:
        g = g.to(device)
        train_loss = compute_train_loss(net, g)
        optim.zero_grad()
        train_loss.backward()
        optim.step()
        epoch_train_loss.append(train_loss.detach().item())
    epoch_train_loss = torch.tensor(epoch_train_loss)
    train_losses.append((epoch, epoch_train_loss.mean()))
        
    # plot
    display.clear_output(wait=True)
    fig, axes = plt.subplots(3, 1, figsize=(10, 8))
    epoch, loss = zip(*train_losses)
    axes[0].plot(epoch, loss)
    
    epoch, loss = zip(*eval_losses)
    axes[1].plot(epoch, loss)
    
    g = loaders['eval'].dataset[2]
    out = net(g, g.ndata['x'].flatten(), g.ndata['y'], g.ndata['mask'].flatten().long())
    attn = net.attn.layer.attn.squeeze(-1).detach()
    attn = attn_to_sparse(g, attn).to_dense()
    x = attn.mean(-1)[:, :12]

    sns.heatmap(x, cmap='binary', xticklabels=list(df.columns), yticklabels=list(df.columns), ax=axes[2])
    plt.show()


g.ndata['y_hat']


import seaborn as sns

def to_nxg(g: dgl.DGLGraph) -> nx.DiGraph:
    nxg = nx.DiGraph(g.to_networkx())
    return nxg
    
def plot_graph(g: dgl.DGLGraph, ax=None, prog='neato', **kwargs):
    nxg = to_nxg(g)
    pos = nx.nx_agraph.pygraphviz_layout(nxg, prog=prog)
    nx.draw(nxg, pos=pos, ax=ax, **kwargs)
    return ax, pos, nxg

def attn_to_sparse(g: dgl.DGLGraph, attn: torch.Tensor):
    n = g.number_of_nodes()
    i = torch.stack(g.edges())
    v = attn
    x = torch.sparse_coo_tensor(i, v.flatten(), (n, n))
    return x


def plot_multihead_attention(g, attn, cmap='binary', node_color='k', node_size=30, prog='neato', scale_width=2., min_width=0.3):
    fig, axes = plt.subplots(3, 4, figsize=(14, 5))
    attn_arr =[]
    for i in range(attn.size(1)):
        ax = axes.flatten()[i]
        ax.set_title("Head {}".format(i))
        a = attn[:, i].detach().flatten()
        _attn = attn_to_sparse(g, a).to_dense().numpy()
        attn_arr.append(_attn)
        sns.heatmap(_attn, ax=ax, linewidths=0, cmap=cmap)
        norm = mpl.colors.Normalize(vmin=0,vmax=1.)
        edge_colors = cm.get_cmap(cmap)(norm(a))
        edges = g.edges()
        edgelist = [(edges[0][i].item(), edges[1][i].item()) for i in range(g.number_of_edges())]
        
        ax, pos, nxg = plot_graph(g, prog=prog, width=a*scale_width + min_width, edge_color=edge_colors, edgelist=edgelist, ax=axes.flatten()[i+4], node_size=node_size, node_color=node_color)
#         nx.draw_networkx_labels(g, ax=ax, pos=pos, labels={v: v for v in list(range(g.number_of_nodes()))})
    plt.show()
    avg = np.stack(attn_arr).mean(0)
    print(avg.shape)
    sns.heatmap(avg, linewidths=0, cmap=cmap)
    return avg, attn_arr
g = eval_loader.dataset[0]
net(g)
net.attn.layer.attn.shape

print(df.columns)
a, arr = plot_multihead_attention(g, net.attn.layer.attn)



a = np.stack(arr).mean(0)
x = a.flatten()

idx = np.argwhere(x > 0.1).flatten()

edges = np.unravel_index(idx, (24, 24))

nxg = nx.DiGraph()

for n1, n2 in zip(*edges):
    node1 = df.columns[n1]
    node2 = df.columns[n2]
    val = a[n1, n2]
    nxg.add_node(n1, name=node1)
    nxg.add_node(n2, name=node2)
    nxg.add_edge(n1, n2, val=val)
    

pos = nx.nx_agraph.pygraphviz_layout(nxg, prog='dot')
labels = {n: ndata['name'] for n, ndata in nxg.nodes(data=True)}
nx.draw(nxg, pos=pos, labels=labels, node_size=300, node_color='w', width=1)



total_loss = 0
for g in eval_loader:
    net.eval()
    with torch.no_grad():
        m = g.ndata['mask'].bool()
        y = net(g)[-1][m]
        y_hat = g.ndata['y_hat'][m].float()
        
        x = torch.stack([y, y_hat]).squeeze(-1)
        sns.heatmap(x.detach().numpy(), cmap='binary')


import random



class LungCancerDataset_OneTarget(Dataset):
    
    def __init__(self, df):
        self.df = df
        self.acquired = {}
        
    def __getitem__(self, i):
        row = df.iloc[i]
        local_causes = []
        for k, v in row.items():
            if v == 1:
                local_causes.append(k)

        target_outcomes = [random.choice(df.columns)]
        local_causes = sorted(list(set(local_causes).union(set(target_outcomes))))

        target_mask = torch.zeros(len(local_causes))
        for i, x in enumerate(local_causes):
            if x == target_outcome:
                target_mask[i] = 1

        g = dgl.graph(([], []))
        g.add_nodes(len(local_causes))
        g.ndata['x'] = torch.tensor([cause_to_idx[c] for c in local_causes]).unsqueeze(1)
        y = torch.tensor(row[target_outcomes].values).unsqueeze(0)
        y = y.expand(g.number_of_nodes(), -1)
        g.ndata['y'] = y
        g.ndata['target_mask'] = target_mask
        add_complete_edges(g)
        assert g.number_of_nodes()
        return g
    
    def __len__(self):
        return len(self.df)



class Network2(nn.Module):
    
    def __init__(self, d_model, h=16, n_heads=4, dropout=0.2):
        super().__init__()
        self.src_embedding = nn.Sequential(
            nn.Embedding(d_model, h),
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.dst_embedding = nn.Sequential(
            nn.Embedding(d_model, h),
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.embedding = nn.Sequential(
            nn.Embedding(d_model, h),
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.attn = AddNorm(h, layer=MultiHeadAttention(h, n_heads), dropout=dropout)
        self.core = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
        )
        self.decode = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, 1)
        )
            
        
    def forward(self, g, n_loops=5):
        with g.local_scope():
            g.ndata['a'] = self.src_embedding(g.ndata['x'].flatten().long())
            g.ndata['b'] = self.dst_embedding(g.ndata['x'].flatten().long())
            g.ndata['h'] = self.embedding(g.ndata['x'].flatten().long())
            out_arr = []

            for i in range(n_loops):
                g.ndata['h'] = self.attn(g.ndata['h'], args=(g, g.ndata['a'], g.ndata['b'], g.ndata['h']))
                g.ndata['h'] = self.core(g.ndata['h'])
                out = nn.Sigmoid()(self.decode(g.ndata['h']))
                out_arr.append(out[g.ndata['target_mask'].bool()])
            return torch.stack(out_arr)
        
net = Network2(32)


dataset = LungCancerDataset_OneTarget(df.iloc[:-100])
loader = DataLoader(dataset, batch_size=32, collate_fn=dgl.batch, shuffle=True)
for x in loader:
    ...
# set out to predict each node in the graph as apposed to just predicting lung cancer
# we are not making any assumptions that lung_cancer is the target which we seek to understand
# rather, we try to learn the entire causal graph
net(x).shape


from tqdm.auto import tqdm
from IPython import display
from matplotlib import pylab as plt

net = Network2(d_model=len(df.columns), h=128, n_heads=8, dropout=0.2)

dataset = LungCancerDataset_OneTarget(df.iloc[:-1000])
loader = DataLoader(dataset, batch_size=256, collate_fn=dgl.batch, shuffle=True, num_workers=12)

eval_dataset = LungCancerDataset_OneTarget(df.iloc[-1000:])
eval_loader = DataLoader(dataset, batch_size=len(eval_dataset), collate_fn=dgl.batch, shuffle=False)

optim = torch.optim.AdamW(net.parameters(), lr=1e-3)
lossfn = torch.nn.BCELoss()
losses = []

def evaluate():
    total_loss = 0
    for g in eval_loader:
        net.eval()
        with torch.no_grad():
            y_arr = net(g)[-1:]
            y_hat = g.ndata['y'][g.ndata['target_mask'].bool()]
            loss = torch.tensor(0.)
            for y in y_arr:
                loss += lossfn(y, y_hat.float())
            
            total_loss+=loss.detach().item()
    return total_loss

eval_losses = []
for epoch in tqdm(range(1000)):
    for g in loader:
        y_arr = net(g)
        y_hat = g.ndata['y'][g.ndata['target_mask'].bool()]
        loss = torch.tensor(0.)
        for y in y_arr:
            loss += lossfn(y, y_hat.float())
        
        optim.zero_grad()
        loss.backward()
        optim.step()

        losses.append(loss.detach().item())
        display.clear_output(wait=True)
        plt.plot(losses)
        plt.show()
        
#         display.clear_output(wait=True)
#         plt.plot(eval_losses)
#         plt.show()
        
    if epoch % 1 == 0:
        eval_loss = evaluate()
        eval_losses.append(eval_loss)

