import numpy as np
import torch
import dgl
from torch import nn


def get_complete_edges(g):
    """List complete edges of a dgl graph"""
    def leading_zero(x):
        yield torch.zeros_like(x[0])
        yield from x
    x = torch.cumsum(g.batch_num_nodes(), 0)
    all_edges = []
    for a, b in zip(leading_zero(x), x):
        edges = torch.combinations(torch.arange(a, b))
        all_edges.append(edges)
    return torch.cat(all_edges, 0).T

def add_complete_edges(g):
    n1 = g.number_of_nodes()
    edges = get_complete_edges(g)
    g.add_edges(edges[0], edges[1])
    g.add_edges(edges[1], edges[0])
    n2 = g.number_of_nodes()
    assert n1 == n2
    return g


import pandas as pd

df = pd.read_csv('lucas0_train.csv')
df.head()


for _, row in df.iterrows():
    ...


from torch.utils.data import DataLoader, Dataset


class LungCancerDataset(Dataset):
    
    def __init__(self, df):
        self.df = df
        self.acquired = {}
        
    def __getitem__(self, i):
        if i not in self.acquired:
            row = df.iloc[i]
            g = dgl.graph(([], []))
            g.add_nodes(len(row))
            add_complete_edges(g)
            n = g.number_of_nodes()
            g.add_nodes(n)
            new_nodes = torch.arange(n, n*2)
            old_nodes = torch.arange(0, n)
            g.add_edges(new_nodes, old_nodes)
            g.ndata['x'] = torch.arange(g.number_of_nodes()).unsqueeze(1)
            g.ndata['y'] = torch.zeros(g.number_of_nodes(), 1)
            m = torch.zeros_like(g.ndata['y'])
            m[torch.randint(0, n, (1,))] = 1
            g.ndata['y_hat'] = g.ndata['y'].clone()
            g.ndata['y'][m.bool()] = 1
            g.ndata['mask'] = m.bool()
            self.acquired[i] = g
            return g
        else:
            g = self.acquired[i]
#             g.ndata['mask'] = torch.ones_like(g.ndata['mask'])
#             g.ndata['mask'][torch.randint(0, 12, (1,))] = 0
#             g.ndata['y'] = g.ndata['y_hat'].clone()
#             g.ndata['y'][~g.ndata['mask'].bool()] = 1
            return g
    
    def __len__(self):
        return len(self.df)


from torch.utils.data import DataLoader, Dataset


class LungCancerDataset(Dataset):
    
    def __init__(self, df):
        self.df = df
        self.acquired = {}
        
    def __getitem__(self, i):
        if i not in self.acquired:
            row = df.iloc[i]
            g = dgl.graph(([], []))
            g.add_nodes(len(row))
            add_complete_edges(g)
            n = g.number_of_nodes()
            g.add_nodes(n)
            new_nodes = torch.arange(n, n*2)
            old_nodes = torch.arange(0, n)
            g.add_edges(new_nodes, old_nodes)
            g.ndata['x'] = torch.arange(g.number_of_nodes()).unsqueeze(1)
            g.ndata['y'] = torch.zeros(g.number_of_nodes(), 1)
            m = torch.zeros_like(g.ndata['y'])
            m[torch.randint(0, n, (1,))] = 1
            g.ndata['y_hat'] = g.ndata['y'].clone()
            g.ndata['y'][m.bool()] = 1
            g.ndata['mask'] = m.bool()
            self.acquired[i] = g
            return g
        else:
            g = self.acquired[i]
#             g.ndata['mask'] = torch.ones_like(g.ndata['mask'])
#             g.ndata['mask'][torch.randint(0, 12, (1,))] = 0
#             g.ndata['y'] = g.ndata['y_hat'].clone()
#             g.ndata['y'][~g.ndata['mask'].bool()] = 1
            return g
    
    def __len__(self):
        return len(self.df)


from abc import ABC, abstractmethod
from torch import nn
from typing import *
import torch
from copy import deepcopy
import matplotlib as mpl
from matplotlib import cm
from dgl import ops as Fops


def clones(net, N):
    return [deepcopy(net) for _ in range(N)]
    
    
class SizedModule(ABC) :

    @abstractmethod
    def get_size(self) -> int:
        ...
                  
class AddNorm(nn.Module):
    
    def __init__(self, size: Optional[int] = None, dropout: float = 0.1, layer: Optional[SizedModule] = None):
        super().__init__()
        if size is None and layer is None:
            return ValueError("Either size or layer must be provided")
        self.size = size or layer.get_size()
        self.layer = layer
        self.norm = nn.LayerNorm(self.size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, args=None, kwargs=None, layer: Optional[SizedModule] = None):
        kwargs = kwargs or dict()
        if args is None:
            args = (x,)
        layer = layer or self.layer
        return self.norm(x + self.dropout(layer(*args, **kwargs)))
    

    
class MultiHeadAttention(nn.Module):
    
    def __init__(self, dim_model, h):
        super().__init__()
        assert dim_model % h == 0
        self.h = h
        self.dim_model = dim_model
        self.d_k = dim_model // h
        self.linears  = clones(nn.Linear(dim_model, dim_model), 4)
        self.attn = None
    
    def _view_head(self, x):
        return x.view(x.size(0), -1, self.h, self.d_k).transpose(1, 2)
        
    def forward(self, g, query, key, value):
        q = self._view_head(self.linears[0](query))
        k = self._view_head(self.linears[1](key))
        v = self._view_head(self.linears[2](value))
        score = Fops.edge_softmax(g, Fops.v_dot_u(g, q, k) / self.d_k**0.5)
#         score = Fops.v_dot_u(g, q, k) / self.d_k**0.5
#         score = F.leaky_relu(Fops.v_dot_u(g, q, k) / self.d_k**0.5)
        out = Fops.u_mul_e_sum(g, v, score)
        out = out.transpose(1, 2).view(g.number_of_nodes(), self.h * self.d_k)
        score = score.view(score.size(0), self.h, -1)
        self.attn = score
        out = self.linears[3](out)
        return out
    

class Network(nn.Module):
    
    def __init__(self, d_model, h=16, n_heads=4, dropout=0.2):
        super().__init__()
        self.src_embedding = nn.Sequential(
            nn.Embedding(d_model, h),
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.dst_embedding = nn.Sequential(
            nn.Embedding(d_model, h),
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.encode = nn.Sequential(
            nn.Linear(1, h),
            nn.LeakyReLU(),
        )
        self.attn = AddNorm(h, layer=MultiHeadAttention(h, n_heads), dropout=dropout)
        self.core = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
        )
        self.decode = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, 1)
        )
            
        
    def forward(self, g, n_loops=5):
        with g.local_scope():
            g.ndata['a'] = self.src_embedding(g.ndata['x'].flatten().long())
            g.ndata['b'] = self.dst_embedding(g.ndata['x'].flatten().long())
            g.ndata['h'] = self.encode(g.ndata['y'].float())
            out_arr = []
            for i in range(n_loops):

                g.ndata['h'] = self.attn(g.ndata['h'], args=(g, g.ndata['a'], g.ndata['b'], g.ndata['h']))
                g.ndata['h'] = self.core(g.ndata['h'])
                out = nn.Sigmoid()(self.decode(g.ndata['h']))
                out_arr.append(out)
            
            return out_arr
        
net = Network(32)


dataset = LungCancerDataset(df.iloc[:-1000])
loader = DataLoader(dataset, batch_size=1, collate_fn=dgl.batch, shuffle=True)
for x in loader:
    ...
m = x.ndata['mask'].bool()
# set out to predict each node in the graph as apposed to just predicting lung cancer
# we are not making any assumptions that lung_cancer is the target which we seek to understand
# rather, we try to learn the entire causal graph
net(x)[-1][~m].shape


from tqdm.auto import tqdm
from IPython import display
from matplotlib import pylab as plt

net = Network(d_model=len(df.columns)*2, h=128, n_heads=4, dropout=0.2)

dataset = LungCancerDataset(df.iloc[:-1000] + df.iloc[:-1000])
loader = DataLoader(dataset, batch_size=256, collate_fn=dgl.batch, shuffle=True)

eval_dataset = LungCancerDataset(df.iloc[-1000:])
eval_loader = DataLoader(dataset, batch_size=len(eval_dataset), collate_fn=dgl.batch, shuffle=False)

optim = torch.optim.AdamW(net.parameters(), lr=1e-3)
lossfn = torch.nn.BCELoss()
losses = []

def evaluate():
    total_loss = 0
    for g in eval_loader:
        net.eval()
        with torch.no_grad():
            m = g.ndata['mask'].bool()
            y_arr = net(g)[-1:]
            
            y_hat = g.ndata['y_hat'][m].float()
            loss = torch.tensor(0.)
            for y in y_arr:
                loss += lossfn(y[m], y_hat.float())
            
            total_loss+=loss.detach().item()
    return total_loss

eval_losses = []
for epoch in tqdm(range(1000)):
    for g in loader:
        m = g.ndata['mask'].bool()
        y_arr = net(g)[-1:]
        y_hat = g.ndata['y_hat'][m].float()
        loss = torch.tensor(0.)
        for y in y_arr:
            loss += lossfn(y[m], y_hat.float())
        print(y_arr[-1][m])
        print(y_hat)
        break
        optim.zero_grad()
        loss.backward()
        optim.step()

#         losses.append(loss.detach().item())
#         display.clear_output(wait=True)
#         plt.plot(losses)
#         plt.show()
        
        display.clear_output(wait=True)
        plt.plot(eval_losses)
        plt.show()
        
    if epoch % 1 == 0:
        eval_loss = evaluate()
        eval_losses.append(eval_loss)
        
        

        


import pandas as pd

df = pd.read_csv('lucas0_train.csv')
df.head()





import seaborn as sns

def to_nxg(g: dgl.DGLGraph) -> nx.DiGraph:
    nxg = nx.DiGraph(g.to_networkx())
    return nxg
    
def plot_graph(g: dgl.DGLGraph, ax=None, prog='neato', **kwargs):
    nxg = to_nxg(g)
    pos = nx.nx_agraph.pygraphviz_layout(nxg, prog=prog)
    nx.draw(nxg, pos=pos, ax=ax, **kwargs)
    return ax, pos, nxg

def attn_to_sparse(g: dgl.DGLGraph, attn: torch.Tensor):
    n = g.number_of_nodes()
    i = torch.stack(g.edges())
    v = attn
    x = torch.sparse_coo_tensor(i, v.flatten(), (n, n))
    return x


def plot_multihead_attention(g, attn, cmap='binary', node_color='k', node_size=30, prog='neato', scale_width=2., min_width=0.3):
    fig, axes = plt.subplots(3, 4, figsize=(14, 5))
    attn_arr =[]
    for i in range(attn.size(1)):
        ax = axes.flatten()[i]
        ax.set_title("Head {}".format(i))
        a = attn[:, i].detach().flatten()
        _attn = attn_to_sparse(g, a).to_dense().numpy()
        attn_arr.append(_attn)
        sns.heatmap(_attn, ax=ax, linewidths=0, cmap=cmap)
        norm = mpl.colors.Normalize(vmin=0,vmax=1.)
        edge_colors = cm.get_cmap(cmap)(norm(a))
        edges = g.edges()
        edgelist = [(edges[0][i].item(), edges[1][i].item()) for i in range(g.number_of_edges())]
        
        ax, pos, nxg = plot_graph(g, prog=prog, width=a*scale_width + min_width, edge_color=edge_colors, edgelist=edgelist, ax=axes.flatten()[i+4], node_size=node_size, node_color=node_color)
#         nx.draw_networkx_labels(g, ax=ax, pos=pos, labels={v: v for v in list(range(g.number_of_nodes()))})
    plt.show()
    avg = np.stack(attn_arr).mean(0)
    print(avg.shape)
    sns.heatmap(avg, linewidths=0, cmap=cmap)
    return avg, attn_arr
g = eval_loader.dataset[0]
net(g)
net.attn.layer.attn.shape

print(df.columns)
a, arr = plot_multihead_attention(g, net.attn.layer.attn)



a = np.stack(arr).mean(0)
x = a.flatten()

idx = np.argwhere(x > 0.1).flatten()

edges = np.unravel_index(idx, (24, 24))

nxg = nx.DiGraph()

for n1, n2 in zip(*edges):
    node1 = df.columns[n1]
    node2 = df.columns[n2]
    val = a[n1, n2]
    nxg.add_node(n1, name=node1)
    nxg.add_node(n2, name=node2)
    nxg.add_edge(n1, n2, val=val)
    

pos = nx.nx_agraph.pygraphviz_layout(nxg, prog='dot')
labels = {n: ndata['name'] for n, ndata in nxg.nodes(data=True)}
nx.draw(nxg, pos=pos, labels=labels, node_size=300, node_color='w', width=1)



total_loss = 0
for g in eval_loader:
    net.eval()
    with torch.no_grad():
        m = g.ndata['mask'].bool()
        y = net(g)[-1][m]
        y_hat = g.ndata['y_hat'][m].float()
        
        x = torch.stack([y, y_hat]).squeeze(-1)
        sns.heatmap(x.detach().numpy(), cmap='binary')


import random



class LungCancerDataset_OneTarget(Dataset):
    
    def __init__(self, df):
        self.df = df
        self.acquired = {}
        
    def __getitem__(self, i):
        row = df.iloc[i]
        local_causes = []
        for k, v in row.items():
            if v == 1:
                local_causes.append(k)

        target_outcomes = [random.choice(df.columns)]
        local_causes = sorted(list(set(local_causes).union(set(target_outcomes))))

        target_mask = torch.zeros(len(local_causes))
        for i, x in enumerate(local_causes):
            if x == target_outcome:
                target_mask[i] = 1

        g = dgl.graph(([], []))
        g.add_nodes(len(local_causes))
        g.ndata['x'] = torch.tensor([cause_to_idx[c] for c in local_causes]).unsqueeze(1)
        y = torch.tensor(row[target_outcomes].values).unsqueeze(0)
        y = y.expand(g.number_of_nodes(), -1)
        g.ndata['y'] = y
        g.ndata['target_mask'] = target_mask
        add_complete_edges(g)
        assert g.number_of_nodes()
        return g
    
    def __len__(self):
        return len(self.df)



class Network2(nn.Module):
    
    def __init__(self, d_model, h=16, n_heads=4, dropout=0.2):
        super().__init__()
        self.src_embedding = nn.Sequential(
            nn.Embedding(d_model, h),
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.dst_embedding = nn.Sequential(
            nn.Embedding(d_model, h),
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.embedding = nn.Sequential(
            nn.Embedding(d_model, h),
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, h),
            nn.LeakyReLU()
        )
        self.attn = AddNorm(h, layer=MultiHeadAttention(h, n_heads), dropout=dropout)
        self.core = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
        )
        self.decode = nn.Sequential(
            nn.Linear(h, h),
            nn.LeakyReLU(),
            nn.Linear(h, 1)
        )
            
        
    def forward(self, g, n_loops=5):
        with g.local_scope():
            g.ndata['a'] = self.src_embedding(g.ndata['x'].flatten().long())
            g.ndata['b'] = self.dst_embedding(g.ndata['x'].flatten().long())
            g.ndata['h'] = self.embedding(g.ndata['x'].flatten().long())
            out_arr = []

            for i in range(n_loops):
                g.ndata['h'] = self.attn(g.ndata['h'], args=(g, g.ndata['a'], g.ndata['b'], g.ndata['h']))
                g.ndata['h'] = self.core(g.ndata['h'])
                out = nn.Sigmoid()(self.decode(g.ndata['h']))
                out_arr.append(out[g.ndata['target_mask'].bool()])
            return torch.stack(out_arr)
        
net = Network2(32)


dataset = LungCancerDataset_OneTarget(df.iloc[:-100])
loader = DataLoader(dataset, batch_size=32, collate_fn=dgl.batch, shuffle=True)
for x in loader:
    ...
# set out to predict each node in the graph as apposed to just predicting lung cancer
# we are not making any assumptions that lung_cancer is the target which we seek to understand
# rather, we try to learn the entire causal graph
net(x).shape


from tqdm.auto import tqdm
from IPython import display
from matplotlib import pylab as plt

net = Network2(d_model=len(df.columns), h=128, n_heads=8, dropout=0.2)

dataset = LungCancerDataset_OneTarget(df.iloc[:-1000])
loader = DataLoader(dataset, batch_size=256, collate_fn=dgl.batch, shuffle=True, num_workers=12)

eval_dataset = LungCancerDataset_OneTarget(df.iloc[-1000:])
eval_loader = DataLoader(dataset, batch_size=len(eval_dataset), collate_fn=dgl.batch, shuffle=False)

optim = torch.optim.AdamW(net.parameters(), lr=1e-3)
lossfn = torch.nn.BCELoss()
losses = []

def evaluate():
    total_loss = 0
    for g in eval_loader:
        net.eval()
        with torch.no_grad():
            y_arr = net(g)[-1:]
            y_hat = g.ndata['y'][g.ndata['target_mask'].bool()]
            loss = torch.tensor(0.)
            for y in y_arr:
                loss += lossfn(y, y_hat.float())
            
            total_loss+=loss.detach().item()
    return total_loss

eval_losses = []
for epoch in tqdm(range(1000)):
    for g in loader:
        y_arr = net(g)
        y_hat = g.ndata['y'][g.ndata['target_mask'].bool()]
        loss = torch.tensor(0.)
        for y in y_arr:
            loss += lossfn(y, y_hat.float())
        
        optim.zero_grad()
        loss.backward()
        optim.step()

        losses.append(loss.detach().item())
        display.clear_output(wait=True)
        plt.plot(losses)
        plt.show()
        
#         display.clear_output(wait=True)
#         plt.plot(eval_losses)
#         plt.show()
        
    if epoch % 1 == 0:
        eval_loss = evaluate()
        eval_losses.append(eval_loss)

