import torch
import dgl
from torch import nn
import torch.nn.functional as F
from dgl import function as fn
import numpy as np
import seaborn as sns
from matplotlib import pylab as plt
from dgl import ops as Fops


# say we have our three linear embeddings of query, key, and value
# it is assumed the dimensions between key and value are equivalent
q = torch.randn(10, 4, 30)
k = torch.randn(10, 3, 30)
v = torch.randn(10, 3, 30)

# Attention is generally applied with QK^TV, often with scaling and softmax applied,
# as in softmax(scaling * QK^T)V
print(torch.matmul(torch.matmul(q, k.transpose(-2, -1)), v).shape)


# to do multihead attention, we split the tensor across multiple heads
# we first calculate some dimensions
batch_size = q.size(0)
d_model = q.size(-1)
heads = 5
d_k = d_model // heads

# else we need to change our embeddings a bit
assert d_model % heads == 0

# then we split d_model across multiple heads
q_view = q.view(batch_size, -1, heads, d_k)

# we then swap the head to the non-matrix (i.e. batch) dimensions using transpose
q_view = q_view.transpose(1, 2)

# NOTE: it must be done in this was as not to juggle the values

# resulting in (batch_size, h, i, d_k)
# our aim is to sum across d_k during matrix multiplication, so that (b, h, i, d_k) * (b, h, d_k, j) -> (b, h, i, j)
# final matmul with the value results in (b, h, i, j) * (b, h, j, d_k) -> (b, h, j, d_k)
q_view = q.view(batch_size, -1, heads, d_k).transpose(1, 2)
k_view = k.view(batch_size, -1, heads, d_k).transpose(1, 2)
v_view = v.view(batch_size, -1, heads, d_k).transpose(1, 2)
    
z = torch.matmul(q_view, k_view.transpose(-2, -1))
print(z.shape)

# the attention as a specific head (e.g. batch=0, head=1) can be found using:
print(z[0, 1])

# out final values with attenion applied
a = torch.matmul(z, v_view)

# which returns us to the original shape
a.view(batch_size, -1, heads * d_k).shape


import dgl
import dgl.function as fn # contains all the built-in optimized message passing and reduce functions.
import networkx as nx
get_ipython().run_line_magic("matplotlib", " inline")

def init_g(d_k):
    g = dgl.graph(([0, 0, 2, 3], [1, 2, 4, 4]))
    n = g.number_of_nodes()
    g.ndata['k'] = torch.randn(n, d_k)
    g.ndata['q'] = torch.randn(n, d_k)
    g.ndata['v'] = torch.randn(n, d_k)
    return g

g = init_g(12)

nxg = nx.DiGraph(g.to_networkx())
print(g)
nx.draw(nxg)


# plotting utils

def to_nxg(g: dgl.DGLGraph) -> nx.DiGraph:
    nxg = nx.DiGraph(g.to_networkx())
    return nxg
    
def plot_graph(g: dgl.DGLGraph, ax=None, prog='neato', **kwargs):
    nxg = to_nxg(g)
    pos = nx.nx_agraph.pygraphviz_layout(nxg, prog=prog)
    nx.draw(nxg, pos=pos, ax=ax, **kwargs)
    return ax, pos, nxg
    
def attn_to_sparse(g: dgl.DGLGraph, attn: torch.Tensor):
    n = g.number_of_nodes()
    i = torch.stack(g.edges())
    v = attn
    x = torch.sparse_coo_tensor(i, v.flatten(), (n, n))
    return x

def plot_attn(g: dgl.DGLGraph, attn: torch.Tensor, ax=None):
    x = attn_to_sparse(g, attn)
    sns.heatmap(x.to_dense().numpy(), ax=ax)

def plot_graph_and_attn(g: dgl.DGLGraph, attn: torch.Tensor):
    fig, axes = plt.subplots(1, 2, figsize=(8, 3))
    axes[0].set_title("Graph")
    axes[1].set_title("Attention")
    plot_graph(g, ax=axes[0], width=attn.flatten().numpy())
    plot_attn(g, attn, ax=axes[1])


def attention(g):
    attention_arrs = []
    score_arrs = []
    
    q = g.ndata['q']
    d_k = q.size(-1)
    scale = d_k**0.5
    n = g.number_of_nodes()
    for i in range(n):
        idx = torch.where(g.edges()[1] == i)[0]
        src = g.edges()[0][idx]
        dst = g.edges()[1][idx]
        src = torch.tensor(list(set(src.tolist())), dtype=torch.long)
        dst = torch.tensor(list(set(dst.tolist())), dtype=torch.long)

        # dst
        q = g.ndata['q'][dst]

        #src
        k = g.ndata['k'][src]
        v = g.ndata['v'][src]
        _z = torch.matmul(q, k.transpose(-2, -1))
        
        score = F.softmax(_z / scale)
        out = torch.matmul(score, v)
        if out.size(0) == 0:
            out = torch.zeros_like(g.ndata['q'][:1])
        attention_arrs.append(out)
        score_arrs.append(score)

    attention = torch.cat(attention_arrs)
    score = torch.cat([s.T for s in score_arrs if s.size(0)], 0)
    return attention, score

g = init_g(3)
out, attn = attention(g)
print(attn)
plot_graph_and_attn(g, attn)


# example: applying basic matmul qk^T in dgl

# initialize graph
g = init_g(3)
print(g.edges())

# lets run attention on edges (2, 4), (3, 4)
i = torch.tensor([0, 1])
src = g.edges()[0][i]
dst = g.edges()[1][i]

# we only look at unique edges
src = torch.tensor(list(set(src.tolist())))
dst = torch.tensor(list(set(dst.tolist())))
print(src, dst)

# filter the data
k = g.ndata['k'][src]
v = g.ndata['v'][src]
q = g.ndata['q'][dst]

# apply matmul in non-graph version
# for mxn, expect a mxm matrix
_z = torch.matmul(q, k.transpose(-2, -1))
print(_z)

# dgl version
with g.local_scope():
    g.apply_edges(fn.v_mul_u('q', 'k', '_z'))
    print('\nEdge Messages: a \'n_edge x d_model\'')
    print(g.edata['_z'])
    print(g.edata['_z'].sum(1))

# typical message passing would take the messages on each edge
# and for every node, sum incoming message. For attention,
# we just want to sum across d_k. We may want to optimize
# this by creating a corresponding kernel to combine this operation.
# We therefore create a new user defined function.
print("\nNote this is different from the typical message passing paradigm, which sums incoming edges.")
with g.local_scope():
    g.update_all(fn.v_mul_u('q', 'k', '_z'), fn.sum('_z', 'score'))
    print(g.ndata['score'])
    
# `v_dot_u` will perform these same operations concisely
print("\nHowever, the most concise was is to use the `v_dot_u` operator, followed by sum as this fuses the kernels")
with g.local_scope():
    g.apply_edges(fn.v_dot_u('q', 'k', '_z'))
    print(g.edata['_z'])


def attention_dgl_gsddmm(g):
    q = g.ndata['q']
    k = g.ndata['k']
    v = g.ndata['v']
    d_k = q.size(-1)
    score = Fops.edge_softmax(g, Fops.v_dot_u(g, q, k) / d_k**0.5)
    out = Fops.u_mul_e_sum(g, v, score)
    return out, score

g = init_g(3)
out0, attn0 = attention(g)
out1, attn1 = attention_dgl_gsddmm(g)

assert torch.allclose(out0, out1)
assert torch.allclose(attn0, attn1)
print(out1)
print(attn1)

plot_graph_and_attn(g, attn0)


def init_complete_g(n_nodes, d_k, half=False):
    edges = torch.combinations(torch.arange(n_nodes), with_replacement=False).T
    edges = edges[:, :]
    
    g = dgl.graph((edges[0], edges[1]))
    if not half:
        g.add_edges(edges[1], edges[0])
    n = g.number_of_nodes()
    x = torch.randn(3, n, d_k) * 2.
    g.ndata['k'] = x[0]
    g.ndata['q'] = x[1]
    g.ndata['v'] = x[2]
    return g

g = init_complete_g(10, 7, half=True)
out, attn = attention_dgl_gsddmm(g)
plot_graph_and_attn(g, attn)


from torch import nn

def torch_multihead_attention(q, k, v, h):
    d_model = k.size(-1)
    d_k = d_model // h
    nbatches = q.size(0)
    assert d_model % h == 0
    assert len(q.shape) == 3
    assert q.size(0) == k.size(0)
    assert q.size(0) == v.size(0)
    assert k.shape == v.shape
    def create_head(x):
        return x.view(nbatches, -1, h, d_k).transpose(1, 2)
    
    q = create_head(q)
    k = create_head(k)
    v = create_head(v)
    
    attn = F.softmax(torch.matmul(q, k.transpose(-2, -1)) * d_k**-0.5)
    out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(nbatches, -1, d_model)
    return out, attn

d_model = 12
q = torch.randn(1, 13, d_model)
k = torch.randn(1, 7, d_model)
v = torch.randn(1, 7, d_model)

out, attn = torch_multihead_attention(q, k, v, 4)
fig, axes = plt.subplots(2, 2, figsize=(7, 5))
for i in range(attn.size(1)):
    ax = axes.flatten()[i]
    sns.heatmap(attn[0, i], ax=ax)
plt.show()

print(out.shape)
sns.heatmap(out[0])


q = torch.randn(10, 12)
k = torch.randn(9, 12)
v = k

g = dgl.graph(([0, 0, 1, 2], [1, 1, 3, 3]))

q = torch.randn(g.number_of_nodes(), 12)
k = torch.randn(g.number_of_nodes(), 12)

def head(x):
    return x.view(x.size(0), -1, 4, 3).transpose(1, 2)

def multihead_attention_dgl_gsddmm(g, h):
    d_model = g.ndata['q'].size(-1)
    def head(x):
        return x.view(x.size(0), -1, h, d_model // h).transpose(1, 2)
    q = head(g.ndata['q'])
    k = head(g.ndata['k'])
    v = head(g.ndata['v'])
    d_k = q.size(-1)
    score = Fops.edge_softmax(g, Fops.v_dot_u(g, q, k) / d_k**0.5)
    out = Fops.u_mul_e_sum(g, v, score)
    out = out.transpose(1, 2).view(g.number_of_nodes(), d_model)
    score = score.view(score.size(0), h, -1)
    return out, score

g = init_complete_g(10, 12, half=True)

out, attn = attention_dgl_gsddmm(g)
print(out.shape)
print(attn.shape)

mh_out, mh_attn = multihead_attention_dgl_gsddmm(g, 4)
print(mh_out.shape)
print(mh_attn.shape)

fig, axes = plt.subplots(2, 2, figsize=(7, 5))
for i in range(mh_attn.size(1)):
    ax = axes.flatten()[i]
    _attn = attn_to_sparse(g, mh_attn[:, i]).to_dense().numpy()
    sns.heatmap(_attn, ax=ax)


from copy import deepcopy
import matplotlib as mpl
from matplotlib import cm

def clones(net, N):
    return [deepcopy(net) for _ in range(N)]
    
def rand_g(n_nodes, n_edges):
    g = dgl.graph(([], []))
    g.add_nodes(n_nodes)
    edges = torch.randint(0, n_nodes, (2, n_edges))
    g.add_edges(*edges)
    return g

class MultiHeadAttention(nn.Module):
    
    def __init__(self, dim_model, h):
        super().__init__()
        assert dim_model % h == 0
        self.h = h
        self.dim_model = dim_model
        self.d_k = dim_model // h
        self.linears  = clones(nn.Linear(dim_model, dim_model), 4)
        self.attn = None
    
    def _view_head(self, x):
        return x.view(x.size(0), -1, self.h, self.d_k).transpose(1, 2)
        
    def forward(self, g, query, key, value):
        q = self._view_head(self.linears[0](query))
        k = self._view_head(self.linears[1](key))
        v = self._view_head(self.linears[2](value))
        score = Fops.edge_softmax(g, Fops.v_dot_u(g, q, k) / self.d_k**0.5)
        out = Fops.u_mul_e_sum(g, v, score)
        out = out.transpose(1, 2).view(g.number_of_nodes(), self.h * self.d_k)
        score = score.view(score.size(0), self.h, -1)
        self.attn = score
        out = self.linears[3](out)
        return out
    
def plot_multihead_attention(g, attn, cmap='binary', node_color='k', node_size=30, prog='neato', scale_width=2., min_width=0.3):
    fig, axes = plt.subplots(2, 4, figsize=(14, 5))
    for i in range(attn.size(1)):
        ax = axes.flatten()[i]
        ax.set_title("Head {}".format(i))
        a = attn[:, i].detach().flatten()
        _attn = attn_to_sparse(g, a).to_dense().numpy()
        sns.heatmap(_attn, ax=ax, linewidths=0, cmap=cmap)
        norm = mpl.colors.Normalize(vmin=0,vmax=1.)
        edge_colors = cm.get_cmap(cmap)(norm(a))
        edges = g.edges()
        edgelist = [(edges[0][i].item(), edges[1][i].item()) for i in range(g.number_of_edges())]
        
        ax, pos, nxg = plot_graph(g, prog=prog, width=a*scale_width + min_width, edge_color=edge_colors, edgelist=edgelist, ax=axes.flatten()[i+4], node_size=node_size, node_color=node_color)
#         nx.draw_networkx_labels(g, ax=ax, pos=pos, labels={v: v for v in list(range(g.number_of_nodes()))})
        
    
def run_multiheadattn_sanity_check():
    model = MultiHeadAttention(12, 4)

    # sanify check
    g = rand_g(20, 100)
    x = torch.randn(g.number_of_nodes(), 12)
    out = model(g, x, x, x)
    attn = model.attn
    plot_multihead_attention(g, attn, prog='neato')
    plt.show()

    g = init_complete_g(20, 12, half=True)
    x = torch.randn(g.number_of_nodes(), 12)
    out = model(g, x, x, x)
    attn = model.attn
    plot_multihead_attention(g, attn, prog='neato')
    plt.show()
    
run_multiheadattn_sanity_check()


class PositionwiseFeedForward(nn.Module):
    "Implements FFN equation."
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.act = nn.ReLU()

    def forward(self, x):
        return self.w_2(self.dropout(self.act(self.w_1(x))))
    
def run_ff_sanity_check(x = torch.randn(10, 12)):
    m = PositionwiseFeedForward(12, 24)
    return m(x)
    
run_ff_sanity_check()


from abc import ABC, abstractmethod
from torch import nn
from typing import *
import torch

class SizedModule(ABC) :

    @abstractmethod
    def get_size(self) -> int:
        ...
                  
class AddNorm(nn.Module):
    
    def __init__(self, size: Optional[int] = None, dropout: float = 0.1, layer: Optional[SizedModule] = None):
        super().__init__()
        if size is None and layer is None:
            return ValueError("Either size or layer must be provided")
        self.size = size or layer.get_size()
        self.layer = layer
        self.norm = nn.LayerNorm(self.size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, args=None, kwargs=None, layer: Optional[SizedModule] = None):
        kwargs = kwargs or dict()
        if args is None:
            args = (x,)
        layer = layer or self.layer
        return self.norm(x + self.dropout(layer(*args, **kwargs)))
    

def run_addnorm_sanity_check(x = torch.randn(10, 12)):
    return AddNorm(size=12, layer=nn.Linear(12, 12))(x)

run_addnorm_sanity_check()


import math

class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)
    
def run_embed_sanity_check():
    lorem = """
    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor 
    incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis 
    nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. 
    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore 
    eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, 
    sunt in culpa qui officia deserunt mollit anim id est laborum.
    """.split()

    vocab = set(lorem)
    n_words = len(vocab)
    vocab_idx = torch.arange(n_words)
    embed = Embeddings(12, n_words)
    return embed(vocab_idx)

run_embed_sanity_check().shape


class EncoderLayer(nn.Module):
    
    def __init__(self, size: int, n_heads: int, d_ff: int, dropout=0.1):
        super().__init__()
        self.attn = MultiHeadAttention(dim_model=size, h=n_heads)
        self.layers = nn.ModuleList([
            AddNorm(size=size, layer=self.attn, dropout=dropout), 
            AddNorm(size=size, layer=PositionwiseFeedForward(d_model=size, d_ff=d_ff, dropout=dropout))
        ])
        
    def forward(self, g, x):
        x = self.layers[0](x, args=(g, x, x, x))
        x = self.layers[1](x)
        return x
    
class Encoder(nn.Module):
    
    def __init__(self, layer: EncoderLayer, N: int):
        super().__init__()
        self.N = N
        self.layers = nn.ModuleList(clones(layer, N))
        
    def forward(self, g, x):
        for l in self.layers:
            x = l(g, x)
        return x
    
def run_encoder_layer_sanity_check():
    model = EncoderLayer(12, 4, 32)
    g = rand_g(20, 40)
    x = torch.randn(g.number_of_nodes(), 12)
    return model(g, x)

def run_encoder_sanity_check():
    model = Encoder(EncoderLayer(12, 4, 32), 4)
    g = rand_g(20, 40)
    x = torch.randn(g.number_of_nodes(), 12)
    return model(g, x)

run_encoder_layer_sanity_check()
run_encoder_sanity_check().shape


class DecoderLayer(nn.Module):
    
    def __init__(self, size: int, n_heads: int, d_ff: int, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(dim_model=size, h=n_heads)
        self.src_attn = MultiHeadAttention(dim_model=size, h=n_heads)
        self.layers = nn.ModuleList([
            AddNorm(size=size, layer=self.self_attn, dropout=dropout), 
            AddNorm(size=size, layer=self.src_attn, dropout=dropout),
            AddNorm(size=size, layer=PositionwiseFeedForward(d_model=size, d_ff=d_ff, dropout=dropout))
        ])
        
    def forward(self, g, x, m):
        x = self.layers[0](x, args=(g, x, x, x))
        x = self.layers[1](x, args=(g,), kwargs=dict(query=x, key=m, value=m))
        x = self.layers[2](x)
        return x
    
class Decoder(nn.Module):
    
    def __init__(self, layer: DecoderLayer, N: int):
        super().__init__()
        self.N = N
        self.layers = nn.ModuleList(clones(layer, N))
        
    def forward(self, g, x, m):
        for l in self.layers:
            x = l(g, x, m)
        return x
    
def run_decoder_layer_sanity_check():
    model = DecoderLayer(12, 4, 32)
    g = rand_g(20, 40)
    x = torch.randn(g.number_of_nodes(), 12)
    m = torch.randn(g.number_of_nodes(), 12)
    return model(g, x, m)

def run_decoder_sanity_check():
    model = Decoder(DecoderLayer(12, 4, 32), 4)
    g = rand_g(20, 40)
    x = torch.randn(g.number_of_nodes(), 12)
    m = torch.randn(g.number_of_nodes(), 12)
    return model(g, x, m)

run_decoder_layer_sanity_check()
run_decoder_sanity_check().shape


max_len = 500
d_model = 12
pe = torch.zeros(max_len, d_model)
pos = torch.arange(0, max_len).unsqueeze(1)
div_term = torch.exp(torch.arange(0, d_model, 2) / d_model * -math.log(10000.0))

pe[:, 0::2] = torch.sin(pos * div_term)
pe[:, 1::2] = torch.cos(pos * div_term)

plt.figure(figsize=(5, 2))
plt.plot(pe.numpy()[:100, 4:7]);


class PositionalEncoding(nn.Module):
    "Implement the PE function."
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model, requires_grad=False)
        pos = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) / d_model * -math.log(10000.0))

        pe[:, 0::2] = torch.sin(pos * div_term)
        pe[:, 1::2] = torch.cos(pos * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        with torch.no_grad():
            pe = self.pe[:, :x.size(1)]
        x = x + pe
        return self.dropout(x)


plt.figure(figsize=(12, 2))
pe = PositionalEncoding(20, 0)
y = pe.forward(torch.zeros(1, 100, 20))
plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())
plt.legend(["dim %d"%p for p in [4,5,6,7]])


class Generator(nn.Module):
    "Define standard linear + softmax generation step."
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)

    def forward(self, x):
        return F.log_softmax(self.proj(x), dim=-1)


# walkthrough of language graph creation

import networkx as nx
import dgl
import torch
import itertools


def to_tuple(x):
    return tuple(x)

def torch_product(a, b):
    return torch.stack([torch.stack(x) for x in itertools.product(torch.arange(a), torch.arange(b))])

def iter_node_types(g):
    visited = set()
    for x in g.canonical_etypes:
        n1, e, n2 = x
        for n in (n1, n2):
            if n not in visited:
                yield n
                visited.add(n)

def create_fake_language_graph(src_size=7, dst_size=5):
    dst_edges = torch.combinations(torch.arange(dst_size)).T
    src_edges = torch.combinations(torch.arange(src_size), with_replacement=True).T
    src_edges = torch.cat([src_edges, torch.flip(src_edges, dims=(0,))], dim=1)
    g = dgl.heterograph({
        ('src', 'src', 'src'): tuple(src_edges),
        ('dst', 'dst', 'dst'): tuple(dst_edges),
        ('src', 'cross', 'dst'): tuple(torch_product(src_size, dst_size).T)
    })
    return g

def hetrograph_to_nxg(g):
    nxg = nx.DiGraph()
    for y, ntype in enumerate(iter_node_types(g)):
        for n in g.nodes(ntype=ntype):
            node = ntype + '_' + str(n.item())
            data = {'id': n, 'ntype': ntype, 'pos': (n.item(), y)}
            nxg.add_node(node, **data)
    for src, etype, dst in g.canonical_etypes:
        edges = torch.stack(g.edges(etype=etype))
        for n1, n2 in edges.T:
            n1 = src + '_' + str(n1.item())
            n2 = dst + '_' + str(n2.item())
            nxg.add_edge(n1, n2, etype=etype)
    return nxg
    

def add_alpha(rgb, alpha):
    colors = []
    for _rgb in rgb:
        if len(_rgb) == 3:
            _rgb = _rgb + (alpha,)
            colors.append(_rgb)
        elif len(_rgb) == 4:
            _rgb = tuple(list(_rgb)[:-1]) + (alpha,)
            colors.append(_rgb)
        else:
            raise ValueError
    return colors

def draw_nxg_with_pos(G,      
                      connectionstyle="arc3,rad=0.1",
                    width=0.1,
                    node_size=100.,
                    linewidths=1,
                      node_fill_alpha=0.3,
                      ax=None):
    scale = 200.
    pos = {n: (d['pos'][0]*scale, d['pos'][1]*2*scale) for n, d in G.nodes(data=True)}
    node_colors = []
    for n, ndata in G.nodes(data=True):
        if ndata['ntype'] == 'src':
            node_color = (0.5, 0, 0)
        elif ndata['ntype'] == 'dst':
            node_color = (0, 0.5, 0.5)
        else: 
            node_color = (0.5, 0.5, 0.5)
        node_colors.append(node_color)

    # nx.draw_networkx_nodes(G, pos)
    nx.draw(
        G, pos,
        connectionstyle="arc3,rad=0.2",
        width=width,
        node_color=add_alpha(node_colors,node_fill_alpha),
        node_size=node_size,
        linewidths=linewidths,
        edgecolors=node_colors,
        ax=ax
    )
    
# Create fake language graph
g = create_fake_language_graph(6, 4)
nxg = hetrograph_to_nxg(g)

# plot the graph
plt.tight_layout()
fig = plt.figure()
ax = fig.gca()
ax.set_title("Language Graph for Translation")
ax.text(-50, 175*2, 'Encoder', horizontalalignment='right', fontsize='large')
ax.text(-50, 0, 'Decoder', horizontalalignment='right', fontsize='large')
ax.text(-50, 100*2, 'Cross\nlanguage\nedges', horizontalalignment='right', verticalalignment='center', fontsize='small')
nxg.add_node('out', ntype='out', pos=(4, 0))
ax.text(800, -100, 'next token', horizontalalignment='center', fontsize='small')
ax.text(0, -100, '<start>', horizontalalignment='center', fontsize='small')
draw_nxg_with_pos(nxg, node_size=200., node_fill_alpha=0.5, width=0.75, ax=ax)
plt.show();

fig, axes = plt.subplots(1, 4, figsize=(8, 2))
for i, (a, b) in enumerate([(4, 3), (5, 4), (3, 6), (7, 3)]): 
    g = create_fake_language_graph(a, b)
    nxg = hetrograph_to_nxg(g)
    ax=axes.flatten()[i]
    ax.set_title('segment {}'.format(i), fontsize='small')
    draw_nxg_with_pos(nxg, node_size=100., node_fill_alpha=0.5, width=0.75, ax=ax)
fig.suptitle("Batch of Segments", fontsize=16, verticalalignment='bottom');


class EncoderDecoder(nn.Module):
    
    def __init__(self, embedding, positional_embedding, encoder, decoder, generator):
        super().__init__()
        self.embedding = embedding
        self.encoder = encoder
        self.decoder = decoder
        self.pos_embed = positional_embedding
        self.generator = generator
        
d_model = 12
h = 4
d_ff = 32
vocab_size = 100
n_encode = 4
n_decode = 4
max_len = 5000
dropout = 0.1
EncoderDecoder(
    encoder=Encoder(EncoderLayer(d_model, h, d_ff, dropout=dropout), n_encode),
    decoder=Decoder(DecoderLayer(d_model, h, d_ff, dropout=dropout), n_decode),
    embedding=Embeddings(d_model, vocab_size),
    positional_embedding=PositionalEncoding(d_model, dropout=dropout, max_len=max_len),
    generator=Generator(d_model, vocab_size)
);
